{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model with BathNormalization & Adam using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **conv-block-model** looks like: (the shape will be different)\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/model/v2/conv-block.png\" height=\"255\" width=\"1000\" /><br>            \n",
    "</div>\n",
    "\n",
    "The whole **model** looks like:\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/model/v2/model.png\" height=\"248\" width=\"1000\" /><br>            \n",
    "</div>\n",
    "\n",
    "The **detail model** :\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/model/v2/model-params.png\" height=\"400\" width=\"800\" /><br>            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the function of defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdvancedModel(X_shape):\n",
    "    \n",
    "    X_input = Input(shape=X_shape, name='X_input')\n",
    "    \n",
    "    # 1st CONV-BLOCK\n",
    "    # Conv2D: strides default (1,1), padding default 'valid'\n",
    "    # Not use bias but center(offset, or named: 'beta')\n",
    "    Z1 = Conv2D(filters=8, kernel_size=(3,3), use_bias=False, kernel_initializer='glorot_uniform', name='conv_1')(X_input) \n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_2')(Z1)\n",
    "    A = Activation('relu', name='relu_3')(Z2)\n",
    "    X = AveragePooling2D(pool_size=(3,3), strides=(1,1), name='avg_pool_4')(A)\n",
    "    \n",
    "    # 2nd CONV-BLOCK\n",
    "    Z1 = Conv2D(filters=12, kernel_size=(5,5), use_bias=False, kernel_initializer='glorot_uniform', name='conv_5')(X) \n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_6')(Z1)\n",
    "    A = Activation('relu', name='relu_7')(Z2)\n",
    "    X = AveragePooling2D(pool_size=(5,5), strides=(1,1), name='avg_pool_8')(A)\n",
    "    \n",
    "    # 3rd CONV-BLOCK\n",
    "    Z1 = Conv2D(filters=16, kernel_size=(5,5), use_bias=False, kernel_initializer='glorot_uniform', name='conv_9')(X) \n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_10')(Z1)\n",
    "    A = Activation('relu', name='relu_11')(Z2)\n",
    "    X = AveragePooling2D(pool_size=(5,5), strides=(1,1), name='avg_pool_12')(A)\n",
    "    \n",
    "    # 4th CONV-BLOCK\n",
    "    Z1 = Conv2D(filters=20, kernel_size=(5,5), use_bias=False, kernel_initializer='glorot_uniform', name='conv_13')(X) \n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_14')(Z1)\n",
    "    A = Activation('relu', name='relu_15')(Z2)\n",
    "    X = AveragePooling2D(pool_size=(5,5), strides=(1,1), name='avg_pool_16')(A)\n",
    "    \n",
    "    # flatten\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 5th NN-BLOCK\n",
    "    Z1 = Dense(units=128, use_bias=False, kernel_initializer='glorot_uniform', name='fc_17')(X)\n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_18')(Z1)\n",
    "    X = Activation('relu', name='relu_19')(Z2)\n",
    "    \n",
    "    # 6th NN-BLOCK\n",
    "    Z1 = Dense(units=64, use_bias=False, kernel_initializer='glorot_uniform', name='fc_20')(X)\n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_21')(Z1)\n",
    "    X = Activation('relu', name='relu_22')(Z2)\n",
    "    \n",
    "    # 7th NN-BLOCK\n",
    "    Z1 = Dense(units=32, use_bias=False, kernel_initializer='glorot_uniform', name='fc_23')(X)\n",
    "    Z2 = BatchNormalization(scale=False, moving_variance_initializer='glorot_uniform', name='bn_24')(Z1)\n",
    "    X = Activation('relu', name='relu_25')(Z2)\n",
    "    \n",
    "    # 8th NN-BLOCK\n",
    "    Y = Dense(units=1, activation='sigmoid', kernel_initializer='glorot_uniform', name='fc_26')(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = Y, name='AdvancedModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a callback to record the losses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LossRecorder(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AdvancedModel(X_shape=(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('datasets/car-v2.h5', 'r', driver='core') as f:\n",
    "    DIVIDE = 6400\n",
    "    X_test, Y_test = f['X'][DIVIDE:], f['Y'][DIVIDE:].reshape(-1, 1)\n",
    "    X_train, Y_train = f['X'][:DIVIDE], f['Y'][:DIVIDE].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape:  (6400, 32, 32, 3)\n",
      "Y_shape:  (6400, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_shape: ', X_train.shape)\n",
    "print('Y_shape: ', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_recorder = LossRecorder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OK, it's time to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.4034 - acc: 0.8342    \n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.2913 - acc: 0.8842    \n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.2583 - acc: 0.8958    \n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.2334 - acc: 0.9077    \n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.2219 - acc: 0.9133    \n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.2159 - acc: 0.9163    \n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.2045 - acc: 0.9186    \n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.1962 - acc: 0.9209    \n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.1962 - acc: 0.9216    \n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 16s - loss: 0.1932 - acc: 0.9234    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5a77d72e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, callbacks=[loss_recorder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s     \n",
      "Final Loss \t = 0.235009089112\n",
      "Test Accuracy \t = 0.91625\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Final Loss \\t = \" + str(preds[0]))\n",
    "print (\"Test Accuracy \\t = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy are both little worse than training. But it's still ok. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Now, let's show the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = len(loss_recorder.losses)\n",
    "plt.plot(range(num), loss_recorder.losses)\n",
    "plt.xlabel('number_of_batch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"images/costs/keras_v2/loss.png\" height=\"500\" width=\"1000\" /><br>            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Check the model archtecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "X_input (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 30, 30, 8)         216       \n",
      "_________________________________________________________________\n",
      "bn_2 (BatchNormalization)    (None, 30, 30, 8)         24        \n",
      "_________________________________________________________________\n",
      "relu_3 (Activation)          (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "avg_pool_4 (AveragePooling2D (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 24, 24, 12)        2400      \n",
      "_________________________________________________________________\n",
      "bn_6 (BatchNormalization)    (None, 24, 24, 12)        36        \n",
      "_________________________________________________________________\n",
      "relu_7 (Activation)          (None, 24, 24, 12)        0         \n",
      "_________________________________________________________________\n",
      "avg_pool_8 (AveragePooling2D (None, 20, 20, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv_9 (Conv2D)              (None, 16, 16, 16)        4800      \n",
      "_________________________________________________________________\n",
      "bn_10 (BatchNormalization)   (None, 16, 16, 16)        48        \n",
      "_________________________________________________________________\n",
      "relu_11 (Activation)         (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "avg_pool_12 (AveragePooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv_13 (Conv2D)             (None, 8, 8, 20)          8000      \n",
      "_________________________________________________________________\n",
      "bn_14 (BatchNormalization)   (None, 8, 8, 20)          60        \n",
      "_________________________________________________________________\n",
      "relu_15 (Activation)         (None, 8, 8, 20)          0         \n",
      "_________________________________________________________________\n",
      "avg_pool_16 (AveragePooling2 (None, 4, 4, 20)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "fc_17 (Dense)                (None, 128)               40960     \n",
      "_________________________________________________________________\n",
      "bn_18 (BatchNormalization)   (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "relu_19 (Activation)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc_20 (Dense)                (None, 64)                8192      \n",
      "_________________________________________________________________\n",
      "bn_21 (BatchNormalization)   (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "relu_22 (Activation)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "fc_23 (Dense)                (None, 32)                2048      \n",
      "_________________________________________________________________\n",
      "bn_24 (BatchNormalization)   (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "relu_25 (Activation)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "fc_26 (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 67,489\n",
      "Trainable params: 66,929\n",
      "Non-trainable params: 560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Also can save the model.**\n",
    "```\n",
    "with open('model.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "```\n",
    "\n",
    "**save the weights**\n",
    "```\n",
    "model.save_weights('trained_params/weights_v2.h5')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
